[{"url":"https://daria-kot.github.io/","title":"About Me","description":null,"body":"Daria Kot\nAbout Me\n\nHi, I'm Daria! I'm currently a computer science student at Cornell University, graduating in Spring 2025.\nI’m passionate about embedded software and robotics-basically anthying that brings hardware and software together to solve cool problems! As a part of the Cornell Rocketry Student Engineering Project Team, I’ve worked on exciting pojects that pushed me to think creatively and work closely with an amazing team. I love learning, building and tackling challenges in a team setting! I’m excited to see where ths journey in robotics and aerospace takes me!\n\n\nPlease explore my page and check out the portfolio section to see what cool projects I’ve worked on!\n","path":null},{"url":"https://daria-kot.github.io/contact/","title":"","description":null,"body":"Contact Me\n\nI’d love to hear from you! Please reach me through the following:\n\nEmail: dak267@cornell.edu\nLinkedIn: Daria Kot\n\n","path":null},{"url":"https://daria-kot.github.io/resume/","title":"My Resume","description":null,"body":"\n\nDownload My Resume\n\n\n","path":null},{"url":"https://daria-kot.github.io/projects/","title":"Projects","description":null,"body":"Welcome to my portfolio!\n","path":null},{"url":"https://daria-kot.github.io/projects/project1/","title":"Project #1","description":null,"body":"This is project #1…\n","path":null},{"url":"https://daria-kot.github.io/projects/project2/","title":"Fast Robots","description":null,"body":"This course focused on systems level design and implementation of dynamic autonomous robots. We designed a fast autonomous car and explored dynamic behaviors, acting forces, sensors, and reactive control on an embedded processor, as well as the benefit of partial off-board computation, low latency software, and noise tolerant implementation.\nTo check out more in depth documentation of developing this car, check out my  Fast Robots Webpage\nHighlights from each Lab\nIn this course, we designed and built a robot car from the ground up, starting with the microcontroller and electronics. The project involved implementing linear and orientation PID controllers, enabling the robot to accurately localize its position and execute path planning. This end-to-end process required integrating hardware, firmware, and software, providing hands-on experience in robotics and control systems.\nLab 1: Setting up the Artemis\nLessons Learned:\n\nSuccessfully Installed and Configured Artemis\nEstablished a communication link between the Artemis (peripherl) and my computer (client)\nCustomized UUIDs to prevent conflicts with other devices\nWrote and ran Python scripts to send nd process commans via BLE.\nDevloped methods for real-time and batch data collection, enabling comparison of responsiveness versus accuracy in data sampling.\n\nLab 2: Setting up the IMU\nLessons Learned:\n\nSuccessfully connected and configured the IMU with Artemis board\nCalculated pitch and roll using acceleration data\nPerformed frequency spectrum analysis to determine a 10Hz cutoff for low-pass filtering, reducing noise in accelerometer data\nDerived pitch, roll, and yaw from gyroscope data\nApplied complementary filter to combine gyroscope and acceleromater data to acheive stability and accuracy in data\nDesigned a loop to collect and process IMU data efficiently\n\n\nUnfiltered gyroscope pitch data\n\nFiltered gyroscope pitch data\nLab 3: Setting up TOF Sensors\nLessons Learned:\n\nAdvanced skills in I2C communication, including dynamic address assignment and multi-sensor integration.\nImproved understanding of the trade-offs between sensor range, accuracy, and performance under real-world constraints.\nDeveloped a robust framework for processing and transmitting data from multiple sensors in real-time.\n\n\nTo check which ranging time worked best for my application I kept the sensor statinary relative to a wall and changed the ranging times to see which interval worked best for my application.\n\nThe measure the repeatability of the sensor I took the average of 1000 readings 5 times at each 10 in distance interval.\nLab 4: Motors and Open Loop Control\n\nGained practical experience with PWM signal generation and motor control.\nIntegrated motors, drivers, and battery into the car chassis.\nRecognized the need for closed-loop feedback to improve the car’s reliability and consistency.\n\n \nWatch the car drive a straight 6ft line after motor calibration.\n\nWatch the car complete a on axis turn\nLab 5: Linear PID control and Linear Interpolation\nLessons Learned:\n\nPractical understanding of PID tuning and challenges such as integrator wind-up and derivative kick.\nImportance of efficient data handling and sampling rates for high-speed controllers.\nEffectiveness of interpolation in managing sensor lag during high-speed robot motion.\n\n\n\nLinear PID controller using non-interpolated TOF data\n\nLinear PID controller using interpolated TOF data\nLab 6: Orientation PID controller\nLessons Learned:\n\nUnderstanding and mitigating gyroscope limitations like drift and maximum rotational velocity is critical for orientation control.\nThe importance of real-time data handling and interactive tuning for flexible, responsive robot behavior.\nCombining proportional, integral, and derivative terms effectively to balance stability, accuracy, and speed of response.\n\n\n\n\nLab 7: Kalman Filter\nLessons Learned:\n\nThe importance of proper initialization and calibration of uncertainty parameters in a Kalman filter to achieve accurate state estimates.\nEffective handling of sensor noise and duplicate data is crucial for maintaining the reliability of the Kalman filter in real-world applications.\nReal-time data processing and adaptive tuning are necessary for the Kalman filter to operate effectively in dynamic environments.\n\n\nLab 8: Stunts!!\nObjective:\nImplement a stunt where the robot would start at a designated point less than 4 meters from a wall, drive forward, and initiate a 180-degree turn when it is within 3 feet from the wall.\n\n\n\n\nLab 9: Mapping\nObjective:\nTo collect accurate distance readings from the TOF (Time-of-Flight) sensor at multiple orientations around the robot to build a map of its environment.\nMethod:\nThe robot was turned in 18-degree increments to collect TOF readings at 20 different orientations. At each orientation, the robot stopped to take the average of 10 TOF readings.\n\n \n Data Collected\n  \n Data points collected in the polar plane\n  \n Data points collected in the global frame\n   \n Data points collected in multiple locations in the map combined in the global frame\nLab 10: Grid Localization using Bayes Filter\nObjective:\nTo implement a Bayes Filter for grid localization, enabling the robot to estimate its position in the map based on sensor measurements, control inputs, and a belief of its location.\n\n Here are videos of the robot’s localization with and without the Bayes Filter. In the first video the odometry model is plotted in red and the actual trajectory of the robot is plotted in green. From the video we can see that the odometry model is really bad at predicting the robots position.\n \n The second video shows how localization with a Bayes Filter produces much better estimates of the robot’s trajectory. Here again the odometry model is plotted in red and the actual trajectory is plotted in green. The estimated trajectory using the Bayes filter is plotted in blue, and is much better at predicting the robots trajectory. The Bayes Filter seems to perform better when the robot is near walls, most likely due to the sensors being more accurate at sensing closer distances.\nLab 11: Localization on The Real Robot\nObjective:\nPerform localization using the Bayes Filter on the actual robot without relying on the prediction step based on an odometry model due to its inaccuracies. The focus was entirely on the update step, which utilizes a 360-degree scan from the ToF sensor to refine the robot’s position estimate.\n\nVideo of the robot performing the observation loop\n \n Results of the localization results (blue points) taken in four points in the map (green points).\nLab 12: Path Planning\nObjective:\nNavigate the robot through a set of waypoints in the map as quickly and accurately as possible using previous lab implementations of localization and orientation control.\n\n\n Shows the robot navigating through waypoints and semi-successfully localizing and moving throught the map.\n","path":null}]