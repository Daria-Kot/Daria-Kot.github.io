[{"url":"https://daria-kot.github.io/projects/","title":"Projects","description":null,"body":"Welcome to my portfolio!\n","path":null},{"url":"https://daria-kot.github.io/projects/project1/","title":"Cornell Rocketry Team","description":null,"body":"\nCurrent Role\nAs the Recovery and Payload Sub-team Lead, I’m guiding a dynamic group of engineers in designing, manufacturing, and testing cutting-edge systems for our high-powered rocket. These include an autonomous recovery system and an air-sampling payload that leverages onboard microfluidics to analyze dust particulates collected during flight. This rocket will compete in the annual Spaceport America Cup, the largest intercollegiate rocketry competition in the world.\nHighlights:\n\n\nLeading the integration of electronics, software, BLiMs, and motors into an L3 test rocket, which collects telemetry data to support the development of a BLiMs control system and validates our new single-event, two-stage recovery scheme.\n\n\nCollaborating with a multidisciplinary team of 50 members to ensure seamless integration across mechanical, electrical, and software systems.\n\n\nDriving recruitment efforts, including organizing information sessions, conducting interviews, and onboarding new members to build a skilled and passionate team.\n\n\nManaging project timelines, defining system requirements, and coordinating testing schedules to ensure successful execution of Recovery and Payload objectives.\n\n\nRecovery and Payload Transporting L3 test rocket to the launch rail!\n \n  \n  Bird's eye view of Rocket's Trajectory\nBreak Line Manipulation System (BLiMs)\nBLiMs is an intelligent recovery system designed to steer parachute break lines and guide the rocket’s descent. As a key developer, I worked on:\n\n\nDesigning an electronic suite with sensors (altimeter, compass, IMU), motor drivers, and microcontrollers.\n\n\nImplementing a closed-loop PID control system to process telemetry data and control stepper motors for real-time adjustments.\n\n\nTesting the system in various conditions to ensure precision and reliability.\n\n\nThis project combined advanced control algorithms with hands-on hardware development, pushing my understanding of embedded systems and aerodynamics.\n\n\nThis is BLiMs!\nPeripheral Systems for Testing BLiMs\nTo support BLiMs development, I created several peripheral systems:\n\n\nRemote Actuator: A wireless system using microcontrollers and radio modules to simulate midair parachute inflation. It enabled precise testing of parachute separation and inflation dynamics.\n\n\nSensor Board: Designed to collect and store telemetry data, this board included an altimeter, compass, IMU, GPS, and an SD card reader.\nThese tools were instrumental in validating the functionality and robustness of BLiMs in real-world scenarios.\n\n\nCamera Stabilization Payload\nStabilizing a camera in a rocket traveling at high speeds is no small feat! I integrated electronics and tuned a PID controller to ensure the camera remained oriented correctly, counteracting the roll of the rocket during flight.\n\n\n\nCompetition Success\nThe culmination of these efforts led to 2nd place out of 160 teams at the 2022-2023 Spaceport America Cup, including a 1st place finish in the 10k Student Research and Development Category. These achievements reflect the incredible teamwork and innovation within Cornell Rocketry.\n\n \nOur 2024 Launch!\n\nTo learn more about the team check out the Cornell Rocketry Website","path":null},{"url":"https://daria-kot.github.io/projects/project2/","title":"Fast Robots","description":null,"body":"This course focused on systems level design and implementation of dynamic autonomous robots. We designed a fast autonomous car and explored dynamic behaviors, acting forces, sensors, and reactive control on an embedded processor, as well as the benefit of partial off-board computation, low latency software, and noise tolerant implementation.\nTo check out more in depth documentation of developing this car, check out my Fast Robots Webpage\nHighlights from each Lab\nIn this course, we designed and built a robot car from the ground up, starting with the microcontroller and electronics. The project involved implementing linear and orientation PID controllers, enabling the robot to accurately localize its position and execute path planning. This end-to-end process required integrating hardware, firmware, and software, providing hands-on experience in robotics and control systems.\nLab 1: Setting up the Artemis\nLessons Learned:\n\nSuccessfully Installed and Configured Artemis\nEstablished a communication link between the Artemis (peripheral) and my computer (client)\nCustomized UUIDs to prevent conflicts with other devices\nWrote and ran Python scripts to send and process commands via BLE.\nDeveloped methods for real-time and batch data collection, enabling comparison of responsiveness versus accuracy in data sampling.\n\nLab 2: Setting up the IMU\nLessons Learned:\n\nSuccessfully connected and configured the IMU with Artemis board\nCalculated pitch and roll using acceleration data\nPerformed frequency spectrum analysis to determine a 10Hz cutoff for low-pass filtering, reducing noise in accelerometer data\nDerived pitch, roll, and yaw from gyroscope data\nApplied complementary filter to combine gyroscope and accelerometer data to achieve stability and accuracy in data\nDesigned a loop to collect and process IMU data efficiently\n\n\nUnfiltered gyroscope pitch data\n\nFiltered gyroscope pitch data\nLab 3: Setting up TOF Sensors\nLessons Learned:\n\nAdvanced skills in I2C communication, including dynamic address assignment and multi-sensor integration.\nImproved understanding of the trade-offs between sensor range, accuracy, and performance under real-world constraints.\nDeveloped a robust framework for processing and transmitting data from multiple sensors in real-time.\n\n\nTo check which ranging time worked best for my application I kept the sensor stationary relative to a wall and changed the ranging times to see which interval worked best for my application.\n\nThe measure the repeatability of the sensor I took the average of 1000 readings 5 times at each 10 in distance interval.\nLab 4: Motors and Open Loop Control\n\nGained practical experience with PWM signal generation and motor control.\nIntegrated motors, drivers, and battery into the car chassis.\nRecognized the need for closed-loop feedback to improve the car’s reliability and consistency.\n\n \nWatch the car drive a straight 6ft line after motor calibration.\n\nWatch the car complete a on axis turn\nLab 5: Linear PID control and Linear Interpolation\nLessons Learned:\n\nPractical understanding of PID tuning and challenges such as integrator wind-up and derivative kick.\nImportance of efficient data handling and sampling rates for high-speed controllers.\nEffectiveness of interpolation in managing sensor lag during high-speed robot motion.\n\n\n\nLinear PID controller using non-interpolated TOF data\n\nLinear PID controller using interpolated TOF data\nLab 6: Orientation PID controller\nLessons Learned:\n\nUnderstanding and mitigating gyroscope limitations like drift and maximum rotational velocity is critical for orientation control.\nThe importance of real-time data handling and interactive tuning for flexible, responsive robot behavior.\nCombining proportional, integral, and derivative terms effectively to balance stability, accuracy, and speed of response.\n\n\n\n\nLab 7: Kalman Filter\nLessons Learned:\n\nThe importance of proper initialization and calibration of uncertainty parameters in a Kalman filter to achieve accurate state estimates.\nEffective handling of sensor noise and duplicate data is crucial for maintaining the reliability of the Kalman filter in real-world applications.\nReal-time data processing and adaptive tuning are necessary for the Kalman filter to operate effectively in dynamic environments.\n\n\nLab 8: Stunts!!\nObjective:\nImplement a stunt where the robot would start at a designated point less than 4 meters from a wall, drive forward, and initiate a 180-degree turn when it is within 3 feet from the wall.\n\n\n\n\nLab 9: Mapping\nObjective:\nTo collect accurate distance readings from the TOF (Time-of-Flight) sensor at multiple orientations around the robot to build a map of its environment.\nMethod:\nThe robot was turned in 18-degree increments to collect TOF readings at 20 different orientations. At each orientation, the robot stopped to take the average of 10 TOF readings.\n\n \n Data Collected\n  \n Data points collected in the polar plane\n  \n Data points collected in the global frame\n   \n Data points collected in multiple locations in the map combined in the global frame\nLab 10: Grid Localization using Bayes Filter\nObjective:\nTo implement a Bayes Filter for grid localization, enabling the robot to estimate its position in the map based on sensor measurements, control inputs, and a belief of its location.\n\n Here are videos of the robot’s localization with and without the Bayes Filter. In the first video the odometry model is plotted in red and the actual trajectory of the robot is plotted in green. From the video we can see that the odometry model is really bad at predicting the robots position.\n \n The second video shows how localization with a Bayes Filter produces much better estimates of the robot’s trajectory. Here again the odometry model is plotted in red and the actual trajectory is plotted in green. The estimated trajectory using the Bayes filter is plotted in blue, and is much better at predicting the robots trajectory. The Bayes Filter seems to perform better when the robot is near walls, most likely due to the sensors being more accurate at sensing closer distances.\nLab 11: Localization on The Real Robot\nObjective:\nPerform localization using the Bayes Filter on the actual robot without relying on the prediction step based on an odometry model due to its inaccuracies. The focus was entirely on the update step, which utilizes a 360-degree scan from the ToF sensor to refine the robot’s position estimate.\n\nVideo of the robot performing the observation loop\n \n Results of the localization results (blue points) taken in four points in the map (green points).\nLab 12: Path Planning\nObjective:\nNavigate the robot through a set of waypoints in the map as quickly and accurately as possible using previous lab implementations of localization and orientation control.\n\n\n Shows the robot navigating through waypoints and semi-successfully localizing and moving through the map.\n","path":null},{"url":"https://daria-kot.github.io/projects/project3/","title":"Predicting Surface Temperature by Green Space Image Recognition","description":null,"body":"Overview\nOur project explored how urban green spaces affect surface temperatures in New York City by leveraging computer vision and artificial intelligence. Using satellite imagery, we developed machine learning models to predict whether a given area’s surface temperature is above or below NYC’s average. This research highlighted the potential of AI in addressing urban heat inequality, a critical issue exacerbated by global warming.\nKey Highlights\nData Collection &amp; Preparation:\n\n\nUtilized satellite imagery and datasets (e.g., NYC heat maps and vegetation indices) to quantify green space and temperature metrics.\nPreprocessed spatial data into image blocks and labels for model training, incorporating geo-spatial attributes like latitude and longitude.\n\n New York City with NDVI mask. Darker green areas correspond to areas with\nmore vegetation.\nCustom AI Models:\n\nBuilt a convolution neural network (CNN) from scratch in PyTorch to classify temperature regions.\nEnhanced the CNN with geo-spatial features, improving prediction accuracy from 78.1% to 84.2%.\nFine-tuned a pre-trained ResNet model for faster training and higher accuracy using transfer learning techniques.\n\n\nCausal Discovery:\n\nApplied causal AI algorithms to explore correlations between green spaces and temperature changes.\nRefined datasets to improve model performance, revealing strong correlations but limited evidence of causation due to data constraints.\n\n\n","path":null},{"url":"https://daria-kot.github.io/","title":"About Me","description":null,"body":"Daria Kot\nAbout Me\n\nHi, I'm Daria! I'm currently a Mechanical Masters of Engineering Student at Cornell University, graduating in Spring 2026. Before persuing an MechE Meng I completed my BS in Computer Science in 2025.\nI’m passionate about embedded software and robotics – essentially anything that combines hardware and software to solve exciting challenges! As a part of the Cornell Rocketry Student Engineering Project Team, I’ve worked on projects that pushed me to think creatively and collaborate closely with an amazing team. My contributions include the development of electronics and software for the Break Line Manipulation System (BLiMs), a remote actuator, and a sensor board. I’ve also integrated and tuned a PID controller for a camera stabilization payload and facilitated the design, manufacturing, and integration of an L3 rocket! These experiences have deepened my technical expertise and strengthened my passion for solving complex problems.\nPlease explore my page and check out the portfolio section to see some of the projects I’ve worked on!\n","path":null},{"url":"https://daria-kot.github.io/resume/","title":"Resume","description":null,"body":"\n\nDownload My Resume\n\n\n","path":null},{"url":"https://daria-kot.github.io/contact/","title":"Contact Me","description":null,"body":"\n\nI’d love to hear from you! Please reach me through the following:\n\nEmail: dak267@cornell.edu\nLinkedIn: Daria Kot\n\n","path":null}]